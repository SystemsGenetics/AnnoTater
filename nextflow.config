/**
 * ===========================
 * AnnoTater Configuration File
 * ===========================
 *
 * This file provides the configuration settings for the FUNC-A workflow.
 */
manifest {
  mainScript = "main.nf"
  defaultBranch = "master"
  nextflowVersion = ">=18.0"
}


params {
  input {
    transcript_fasta = "${PWD}/examples/Citrus_sinensis-orange1.1g015632m.g.fasta"
    protein_fasta = ""
    // This should just be the data files not the entire iprscan program + data.
  }
  data {
    interproscan = "/local/dbs/interproscan/interproscan-5.36-75.0/data"
    // Current version requires Panther 14.1
    panther = "/local/dbs/panther/panther"
    nr = "/local/dbs/nr"
    sprot = "/local/dbs/uniprot_sprot"
    orthodb {
       data_path = "/local/dbs/orthodb"
       species = [3702]
       dbs {
         arthropoda = false
         verebrata = false
         protozoa = false
         bacteria = false
         fungi = false
         plants = true
         virdae = false
       }
    }
  }
  output {
    dir = "${PWD}/output"
  }
  execution {
    queue_size = 2
  }
  steps {
    orthodb {
      enable = true
    }
    dblastx_sprot {
      enable = true
    }
    dblastx_nr {
      enable = false
    }
    interproscan {
      enable = true
      // A comma-separated list of applications that InterProscan should run.
      applications = "TIGRFAM,SFLD,SUPERFAMILY,Gene3D,Hamap,Coils,ProSiteProfiles,SMART,CDD,PRINTS,Pfam,MobiDBLite,PIRSF,PANTHER,ProDom"
    }
  }
}

report {
  file = "${params.output.dir}/report.html"
}


timeline {
  file = "${params.output.dir}/timeline.html"
}

trace {
  fields = "task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes"
  file = "${params.output.dir}/trace.txt"
  raw = true
}

singularity {
  autoMounts = true
  cacheDir = "/local/singularity"
  runOptions = "-B ${params.data.interproscan}:/usr/local/interproscan/data,${params.data.panther}:/usr/local/interproscan/data/panther,${params.data.nr}:/annotater/nr,${params.data.sprot}:/annotater/uniprot_sprot,${params.data.orthodb.data_path}:/annotater/orthodb"
}

process {
  container = "annotater/base:0.9"
  errorStrategy = "retry"
  maxRetries = 3

  withLabel:interproscan {
    container = "annotater/interproscan:5.36"
    time = "24h"
    // Interproscan always requires a master and a worker process.
    cpus = 2
  }
  withLabel:ncbi_blast {
    container = "annotater/ncbi-blast:2.2.31"
  }
  withLabel:diamond {
    container = "annotater/diamond:0.9.25"
  }
  withLabel:diamond_makedb {
    container = "annotater/diamond:0.9.25"
    maxRetries = 0
    cpus = 2
  }

}

profiles {
  //
  // The standard profile uses the local executor, in which processes are simply
  // launched as normal processes on the local machine. By default the local
  // executor uses the number of CPU cores to limit how many processes are run
  // in parallel.
  //
  standard {
    process.executor = "local"
    executor {
      queueSize = "${params.execution.queue_size}"
    }
  }

  // A profile for testing on the Travis.CI testing platform.
  travis {
    docker.enabled = true
    process {
      withLabel:retry {
        errorStrategy = "terminate"
      }
    }
  }

  //
  // The testing profile is used to override the execution params and force the
  // workflow to terminate immediately if any process fails, which is useful for
  // debugging purposes.
  //
  testing {
    process {
      withLabel:retry {
        errorStrategy = "terminate"
      }
    }
  }


  //
  // The k8s profile provides basic execution settings for running the workflow
  // on a Kubernetes cluster.
  //
  k8s {
    process {
      executor = "k8s"
      cpus = 1
      memory = 1.GB

      withLabel:multithreaded {
        cpus = 8
        memory = 8.GB
      }
    }
    executor {
      queueSize = "${params.execution.queue_size}"
    }
  }

  //
  // Clemson's Palmetto cluster uses the PBS scheduler. Here we provide
  // an example for execution of this workflow on Palmetto with some
  // defaults for all steps of the workflow.
  //
  pbs {
    process {
      executor = "pbspro"
      time = "8h"
      cpus = 2
      memory = 2.GB

      withLabel:multithreaded {
        cpus = 8
        memory = 8.GB
      }
    }
    executor {
      queueSize = "${params.execution.queue_size}"
    }
  }

  //
  // WSU's Kamiak cluster uses the SLURM scheduler. Here we provide
  // an example for execution of this workflow on Kamiak with some
  // defaults for all steps of the workflow.
  //
  slurm {
    process {
      executor = "slurm"
      queue = "ficklin"
      time = "24h"
      cpus = 1

      withLabel:multithreaded {
        cpus = 1
      }
    }
    executor {
      queueSize = "${params.execution.queue_size}"
    }
  }
}
