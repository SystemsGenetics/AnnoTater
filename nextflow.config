/**
 * ===========================
 * AnnoTater Configuration File
 * ===========================
 *
 * This file provides the configuration settings for the FUNC-A workflow.
 */
manifest {
  mainScript = "main.nf"
  defaultBranch = "master"
  nextflowVersion = ">=18.0"
}


params {
  input {
    fasta_file = "${PWD}/examples/Citrus_sinensis-orange1.1g015632m.g.fasta"
    type = "nuc"
  }
  data {
    interproscan = "/local/dbs/interproscan/interproscan-5.36-75.0/data"
    // Current version requires Panther 14.1
    panther = "data/panther"
    nr = "data/nr"
    sprot = "data/uniprot_sprot"
    orthodb = "data/orthodb"
    string = "data/string"
    trembl = "data/uniprot_trembl"
  }
  output {
    dir = "${PWD}/output"
  }
  execution {
    queue_size = 2
  }
  steps {
    orthodb {
      enable = false
      levels = [3702]
      db = "plants"
    }
    dblast_sprot {
      enable = true
    }
    dblast_trembl {
      enable = true
    }
    dblast_nr {
      enable = false
    }
    interproscan {
      enable = true
      // A comma-separated list of applications that InterProscan should run.
      applications = "TIGRFAM,SFLD,SUPERFAMILY,Gene3D,Hamap,Coils,ProSiteProfiles,SMART,CDD,PRINTS,Pfam,MobiDBLite,PIRSF,PANTHER,ProDom"
    }
    string {
      enable = true
    }
  }
}

report {
  file = "${params.output.dir}/report.html"
}


timeline {
  file = "${params.output.dir}/timeline.html"
}

trace {
  fields = "task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes"
  file = "${params.output.dir}/trace.txt"
  raw = true
}

docker {
  fixOwnership = true
  runOptions = "--init"
}

singularity {
  autoMounts = true
  cacheDir = "/local/singularity"
  runOptions = "-B ${params.data.interproscan},${params.data.panther},${params.data.nr},${params.data.sprot},${params.data.orthodb},${params.data.string},${params.data.trembl}"
}

process {
  container = "annotater/base:0.9"
  errorStrategy = "retry"
  maxRetries = 3

  withLabel:interproscan {
    container = "annotater/interproscan:5.36-0.9"
    time = "24h"
    // Interproscan always requires a master and a worker process.
    cpus = 2
  }
  withLabel:ncbi_blast {
    container = "annotater/ncbi-blast:2.2.31-0.9"
  }
  withLabel:orthdb_level2species {
    container = "annotater/python:3.7-0.9"
  }
  withLabel:diamond {
    container = "annotater/diamond:0.9.25-0.9"
  }
  withLabel:diamond_makedb {
    container = "annotater/diamond:0.9.25-0.9"
    maxRetries = 0
    cpus = 2
  }
  withLabel:interproscan_combine {
    container = "annotater/python:3.7-0.9"
  }
}

profiles {
  docker {
    docker.enabled = true
  }
  //
  // The standard profile uses the local executor, in which processes are simply
  // launched as normal processes on the local machine. By default the local
  // executor uses the number of CPU cores to limit how many processes are run
  // in parallel.
  //
  standard {
    process.executor = "local"
    executor {
      queueSize = "${params.execution.queue_size}"
    }
  }

  // A profile for testing on the Travis.CI testing platform.
  travis {
    docker.enabled = true
    process {
      withLabel:retry {
        errorStrategy = "terminate"
      }
    }
  }

  //
  // The testing profile is used to override the execution params and force the
  // workflow to terminate immediately if any process fails, which is useful for
  // debugging purposes.
  //
  testing {
    process {
      withLabel:retry {
        errorStrategy = "terminate"
      }
    }
  }


  //
  // The k8s profile provides basic execution settings for running the workflow
  // on a Kubernetes cluster.
  //
  k8s {
    process {
      executor = "k8s"
      cpus = 1
      memory = 1.GB

      pod = [
        volumeClaim:'deepgtex-prp',
        mountPath: '/annotater',
        subPath: 'workspace/alucinor/dbs'
      ]

      withLabel:multithreaded {
        cpus = 8
        memory = 8.GB
      }
    }
    executor {
      queueSize = "${params.execution.queue_size}"
    }
  }

  //
  // Clemson's Palmetto cluster uses the PBS scheduler. Here we provide
  // an example for execution of this workflow on Palmetto with some
  // defaults for all steps of the workflow.
  //
  pbs {
    process {
      executor = "pbspro"
      time = "8h"
      cpus = 2
      memory = 2.GB

      withLabel:multithreaded {
        cpus = 8
        memory = 8.GB
      }
    }
    executor {
      queueSize = "${params.execution.queue_size}"
    }
  }

  //
  // WSU's Kamiak cluster uses the SLURM scheduler. Here we provide
  // an example for execution of this workflow on Kamiak with some
  // defaults for all steps of the workflow.
  //
  slurm {
    process {
      executor = "slurm"
      queue = "ficklin"
      time = "24h"
      cpus = 1

      withLabel:multithreaded {
        cpus = 1
      }
    }
    executor {
      queueSize = "${params.execution.queue_size}"
    }
  }
}
